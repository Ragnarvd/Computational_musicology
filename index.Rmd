---
title: "Computational Musicology"
author: "Ragnar van Dort"
date: "`r Sys.Date()`"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: flatly
    css: styles.css
    self_contained: false

---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Modules to import
library(dplyr)
library(ggplot2)
library(tidyverse)
library(plotly)
library(spotifyr)
library(compmus)
library(grid)
library(gridExtra)
library(ggpubr)
library(cowplot)
library(showtext)
library(ggdendro)
library(heatmaply)
library(tidymodels)
# Custom font
font_add("TTNorms", "TTNorms-ExtraBold.otf")
showtext_auto()
```

```{r, include=FALSE}
# Load corpus playlist from Spotify API
corpus <- get_playlist_audio_features("", "7ex5hFjCiAdaXy6FFoe2Vi") %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major")) %>% 
  mutate(artists.name = map_chr(track.album.artists, function(x) x$name[1]))
```

```{r, include=FALSE}
# Define your own theme function below
theme_study <- function() {
    theme_minimal() +
    theme(
      text = element_text(family = "TTNorms", color = "#7395AE", size=14),
      plot.title = element_text(hjust = 0.5),
      plot.background = element_rect(fill = "#181818", colour="#181818"),
      panel.background  = element_rect(fill = "#989898"),
      legend.key = element_rect(fill = "#7395AE"),
      #legend.background = element_rect(fill = "#131313")
      panel.grid = element_line(color = "#7395AE")
    )
}
```
### The best music for studying {data-commentary-width=400}

**The best music for studying**

The main goal of this corpus is to compare playlists, to be specific, compare study playlists. A lot of people study, and a lot of people study while listening to music. All people are different and we also see this in their music taste; it's often said that classical music is great for studying and this is also backed by different studies but there also a lot of other kinds of music that students listen to: LoFi, techno etc.

Over the years I created a study playlist myself, it consist of roughly 3 genres I think: some songs with text but they are not to energetic and are a bit "slow", some techno music and some classical music. I highly doubt it's te the best playlist for studying, but over the years I know that I studied with these songs on. Maybe some songs are not ideal itself to listen to during studying but they help me get in a study mode. Maybe good to know due to it's variety its more a playlist that I use as a storage space for study music than that I actually play the playlist while studying. 


In this corpus I will compare my study playlist with some popular spotify study playlists, I'm curious how different my playlist is compared to a "normal" study playlist. 

I will work with four different playlists all created by spotify: a pop study playlist, a classical music study playlist, a techno study playlist and a LoFi study playlist.

I will compare my own study playlist with some popular spotify playlists. For the best comparison I decided to take three spotify playlists with the genres that are also mainly represented in my own playlist:

- Pop Study (sudy music with text): https://open.spotify.com/playlist/37i9dQZF1DWSoyxGghlqv5 
- Classical Studying music: https://open.spotify.com/playlist/37i9dQZF1EIgLoMVUd9oTU 
- Electronic Focus: https://open.spotify.com/playlist/37i9dQZF1DX0wMD4IoQ5aJ 
- My own study music playlist: https://open.spotify.com/playlist/7ex5hFjCiAdaXy6FFoe2Vi

My hypothesis for this research is that there will be a lot of similarities, probably a low bpm and not too energetic for example. But I hope to find some interesting differences that I can then investigate further and possibly explain.

There are a few interesting and atypecal tracks in the playlist. There are of course some tracks that feature in my own playlist and also in spotify's playlist (Una mattine, Comptine d'un autre été: L'après-midi) altough they are a different version. Maybe some atypical music are the movie soundtracks that are in my playlist like: Time-Hans zimmer and the Godfather finale- Nino Rotta.

*** 

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/7ex5hFjCiAdaXy6FFoe2Vi?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1EIgLoMVUd9oTU?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX0wMD4IoQ5aJ?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DWSoyxGghlqv5?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
### Track level features

#### __The Tempo and Instrumentalness in study music__

```{r tempo and intrumentalness, echo=FALSE}
pop <- get_playlist_audio_features("", "37i9dQZF1DWSoyxGghlqv5")
classic <- get_playlist_audio_features("", "37i9dQZF1EIgLoMVUd9oTU")
electronic <- get_playlist_audio_features("", "37i9dQZF1DX0wMD4IoQ5aJ")
studying <- get_playlist_audio_features("", "7ex5hFjCiAdaXy6FFoe2Vi")

study <-
  pop %>%
  mutate(country = "Pop study") %>%
  bind_rows(classic %>% mutate(country = "Classical study")) %>%
  bind_rows(electronic %>% mutate(country = "Electronic study")) %>%
  bind_rows(studying %>% mutate(country = "Stu(dying)"))%>%
  mutate(
    country = fct_relevel(country, "Pop study, Classical study, Electronical study", "My study playlist")
  )

february_dip <-
  study %>%
  ggplot(                          # Set up the plot.
    aes(
      x = tempo,
      y = instrumentalness,
      size = track.popularity,
      colour = energy,
      label = track.name           # Labels will be interactively visible.
    )
  ) +
  geom_point() +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~country) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis. Outliers are not 
    limits = c(0, 200),
    breaks = c(0, 100, 200),        # Use grid-lines for quadrants only.
    minor_breaks = c(25, 50, 75, 125, 150, 175)            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
    guide = "none"
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
    guide = "none"                 # Remove the legend for size.
  ) +
  theme_study() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Tempo",
    y = "Instrumentalness"
  )

ggplotly(february_dip)
```

There is no consensus in the literature about what music is best for studying but every answer has one thing in common; the main goal is that it should help you to focus. Generally speaking music with a relative low bpm (<120 bpm) is best suited for this. In study music (new) texts also seem to be distractive, so a high instrumentalness would be a good sign. 

I plotted the instrumentalness on the y-axis, the tempo on the x-axis, the color is the energy of the song and the size is its popularity. 

The plots are kind of what I expected. The Spotify music playlists are pretty clustered while my own study playlist is not so much. This is not really hard to explain. The Spotify playlists consist of one genre while my playlist is a mix of genres. 

#### __The valence in the different playlistst__

```{r some plots, echo=FALSE}
library(dplyr)
library(spotifyr)
library(plotly)
library(ggplot2)

pop <- get_playlist_audio_features("", "37i9dQZF1DWSoyxGghlqv5")
classic <- get_playlist_audio_features("", "37i9dQZF1EIgLoMVUd9oTU")
electronic <- get_playlist_audio_features("", "37i9dQZF1DX0wMD4IoQ5aJ")
studying <- get_playlist_audio_features("", "7ex5hFjCiAdaXy6FFoe2Vi")

study <-
  pop %>%
  mutate(country = "Pop study") %>%
  bind_rows(classic %>% mutate(country = "Classical study")) %>%
  bind_rows(electronic %>% mutate(country = "Electronic study")) %>%
  bind_rows(studying %>% mutate(country = "Stu(dying)"))%>%
  mutate(
    country = fct_relevel(country, "Pop study, Classical study, Electronical study", "Stu(dying)")
  )
green <- "#1ed760"
yellow <- "#e7e247"
pink <- "#ff6f59"
blue <- "#17bebb"


viz4 <- ggplot(study, aes(x=valence, fill=playlist_name,
                            text = paste(playlist_name)))+
  geom_density(alpha=0.7, color=NA)+
  scale_fill_manual(values=c(green, yellow, pink, blue))+
  labs(x="Valence", y="Density") +
  guides(fill=guide_legend(title="country"))+
  theme_study()+
  ggtitle("Distribution of Valence Data")

ggplotly(viz4, tooltip=c("text"))

```
Valence is a Spotify feature that describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). While study music is not known for its valence it is still interesting to take a look at it. I compare different kinds of playlists so it is interesting to see which kind of study music is the most positive. 

**Analysis**
As is clearly visible, Spotify classifies classical music with a very low valence, and as expected pop music has the highest valence. My own study playlist (combination of the different genres) is the most equally distributed. 


### Comptine d'un autre été: L'après-midi[chroma features].

```{r tallis, echo=FALSE}
lavinia <-
  get_tidy_audio_analysis("06MWWxWRuBtYxcJP71tN0q") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
tierssen <-
  get_tidy_audio_analysis("14rZjW3RioG7WesZhYESso") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
maria_dist <-
  compmus_long_distance(
    lavinia %>% mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
    tierssen %>% mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
    feature = pitches,
    method = "cosine"
  )
```

```{r tallis-plot, echo=FALSE}
maria <-
  maria_dist %>%
  mutate(
    lavinia = xstart + xduration / 2,
    tierssen = ystart + yduration / 2
  ) %>%
  ggplot(
    aes(
      x = lavinia,
      y = tierssen,
      fill = d
    )
  ) +
  geom_tile(aes(width = xduration, height = yduration)) +
  coord_fixed() +
  scale_x_continuous(
    breaks = c(0, 11, 29, 45, 65, 82, 100, 119, 126),
    labels =
      c(
        "Melody left hand",
        "melody 1 ",
        "Melody 2",
        "Melody",
        "Melody 1, octave higher",
        "Melody 2, octave higher",
        "Melody 3, octave higher",
        "fades out",
        ""
      ),
  ) +
  scale_y_continuous(
    breaks = c(0, 11, 31, 49, 69, 90, 110, 131, 140),
    labels =
      c(
        "Melody left hand",
        "melody 1 ",
        "Melody 2",
        "Melody",
        "Melody 1, octave higher",
        "Melody 2, octave higher",
        "Melody 3, octave higher",
        "fades out",
        ""
      ),
  ) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_study() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x = "Lavinia meijers", y = "Yann Tierssen")

gt <- ggplotGrob(maria)
grid.newpage()
grid.draw(rectGrob(gp = gpar(fill = "#181818", col = "#181818")))
grid.draw(gt)
```

***
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/06MWWxWRuBtYxcJP71tN0q?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

**Comparing two versions of Comptine d'un autre été - l'après midi**

This is a visualization of two performances of the piano piece " Comptine d'un autre été - l'après midi" a performance of Lavina meijers, and the original performance by Yann Tierssen. I chose this music piece because the version of Lavina Meijers was in my playlist, and the version by Yann Tierssen was in the Classical study music playlist. 

It's the soundtrack of the French film Amelie and became popular ever since. It's a piano piece and notice that the left melody of the left hand part never changes (except from intensity). The right hand part has three distinctive melodies in a row that have different characteristics: The first one is catchy and has some mild dissonance, the second has fewer notes and leaves more space, and the third is a flurry of repeated triplets. Halfway through the song, the right hand jumps up an octave and plays the exact same thing as it did in the first half.

The version of Yann Tierssen is 16 seconds longer than the version of lavina meijers (2:22, 2:06). I have to look at his again but I can't distinguish this immediately. I think this is not that that the version of Lavina is a bit faster but its more that it starts earlier and has last pause between the different pieces of the music/less fade. This explains the repetition that we can clearly see in the plot.

### The Godfather finale [chroma and timbre features]

```{r godfather, echo=FALSE}
bzt <-
  get_tidy_audio_analysis("78Q2Em989i9lagCBf8XGyX") %>% 
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "mean"
      )
  )
godfather_plot <- bind_rows(
  bzt %>% 
    compmus_self_similarity(pitches, "cosine") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  bzt %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_study() + theme(strip.text = element_text(color = "#7395AE"), panel.background  = element_rect(fill = "#121212", color="#121212")) + 
  labs(x = "", y = "", title = "The godfather Finale - Nino Rota")

gt <- ggplotGrob(godfather_plot)
grid.newpage()
grid.draw(rectGrob(gp = gpar(fill = "#181818", col = "#181818")))
grid.draw(gt)

```

***
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/78Q2Em989i9lagCBf8XGyX?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

**About the track**

The Godfather Finale is an iconic track which brings nostalgia to everyone that listens to it. 
As all music in the godfather it’s composed by Nino Rota,  a famous composer before working with Coppola in the Godfather. His works varies from operas, orchestral works, ballets, choral and chamber works to music. The finale is the same as the Godfather's opening walze, except this time the trumpet is not lonely, but there is a whole orchestra. 

**Analysis**

The two self-similarity matrices at the left, each summarized at the bar level but with axes in seconds, illustrate pitch- and timbre-based self-similarity within Nini Rotta’s  famous The Godfather Finale. The chroma-based matrix shows us that the piece exists of two parts, divided around 110 sec. But furthermore the chroma similarities seem to be all over the place, although you can distinguish the violin part (110-160 sec). This part is also very visible in the Timbre-based matrix.

### Chordogram and Keygram Time Hans Zimmer. {data-commentary-width="400"}


```{r chordogram, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("6ZFbXIJkuI1dVNWvzJzown") %>%
  compmus_align(beats, segments) %>%
  select(beats) %>%
  unnest(beats) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan" # summary, norm
      )
  )
chordogram <- twenty_five %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "manhattan",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_study() + theme(strip.text = element_text(color = "#7395AE"), panel.background  = element_rect(fill = "#121212", color="#121212")) + 
  labs(x = "Time (s)", y = "", title = "Chordogram Time - Hans Timmer")

# over het algemeen, hoe meer geel hoe duidelijker een akkoord wordt herkend, blauw is slecht herkennen
# 

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
twenty_five <-
  get_tidy_audio_analysis("6ZFbXIJkuI1dVNWvzJzown") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan" # summary, norm
      )
  )
keygram <- twenty_five %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "manhattan",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_study() + theme(strip.text = element_text(color = "#7395AE"), panel.background  = element_rect(fill = "#121212", color="#121212")) + 
  labs(x = "Time (s)", y = "", title = "Keygram Time - Hans Timmer")

chordogram
keygram
```

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6ZFbXIJkuI1dVNWvzJzown?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

**About the track**

Time from Hans Timmer is the soundtrack from the famous film Inception by Christopher Nolan. The track perfectly captures the essence of time itself. Time is an ever receding pocket of opportunity that we take advantage of to develop purpose in our lives. On the right there are two plots, a chordogram and a Keygram. The piece is segmented according to Spotify’s estimates, and the distances represented are Manhattan distances from Spotify’s chroma vectors to key and chord profiles. 

**Analysis**

In general a dark blue color is a good recognition and a bright yellow color states that there is no recognition possible. The plots are both a bit blurry but in the chordogram we can see a figure that looks a bit like a sinus. This is explained by the fact that there is a lot of repetition in the song, the same melody is repeated in the song every time just a bit different. The recognition is best at the end of the song (+- 250 sec) this is due to the fact that at this part the song fades out and the melody is slowly repeated without a lot of noise, just a pure sound. The recognition from C-major to D-major is more or less correct.  In the keygram the key estimates are correct in general, most of the song is understood as G-major. While it is originally written in E-minor the two keys hold the exact same nodes and G major is the relative major of E minor, and E minor is the relative minor of G major. So we don’t blame the algorithm.

### Tempogram {data-commentary-width="460"}

```{r echo=FALSE}
p1 <- get_tidy_audio_analysis("3weNRklVDqb4Rr5MhKBR3D") %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Nuvole bianche - Ludovico Einaudi") +
  theme_study()
ggnexus <- 
p2 <- get_tidy_audio_analysis("2aJDlirz6v2a4HREki98cP") %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Glue - Bicep") +
  theme_study()
subplot(ggplotly(p1), ggplotly(p2), nrows = 2, margin = 0.05, shareX = TRUE, 
        shareY = TRUE, titleY = TRUE) %>% layout(title = '') %>% 
  layout(annotations = 
           list(list(x = 0.5 , y = 1.07, text = "Nuvole bianche - Ludovico Einaudi",
                     showarrow = F, xref='paper', yref='paper'),
                list(x = .5 , y = .49, text = "Glue - Bicep",
                     showarrow = F, xref='paper', yref='paper'))
)
```

***

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3weNRklVDqb4Rr5MhKBR3D?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2aJDlirz6v2a4HREki98cP?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

**Track Information**

For my Fourier-Based tempogram I used two songs from my study playlist, both completely different. A classical piano (Nuvole Bianche) song and a techno song (Glue). I listen to them both while studying but in completely different situations. The piano song helps me to focus and get calm, while the techno song helps me to focus and gives me some energy (for a late night etc.). It’s nice to compare the two because they are so different but also have very similar features such as the tempo. The piano has a tempo of 136 BPM, the techno 132 BPM. Both are a bit high for study music but I did not notice this before investigating the tempo. 

**Analysis**

We can clearly see that the algorithm has a lot more difficulties with the piano track than with the techno track. The techno track is correctly estimated at 132 BPM and the piano track is a bit messy. Two parts are impossible to estimate, both parts only consist of slowly played chords that fade out and there are parts of silence which give the yellow lines. The “lines” are the piano melody. I also hear an acceleration in these parts but I don’t know if this is due to a higher loudness, more nodes etc. Or to a higher BPM as is estimated. My musical knowledge is not sufficient enough for this. But stats on the internet say that the whole track is played at 136 BPM. 

### Machine Learning using KNN {data-commentary-width="400"}

```{r echo=FALSE}

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  

pop <- get_playlist_audio_features("", "37i9dQZF1DWSoyxGghlqv5")
classic <- get_playlist_audio_features("", "37i9dQZF1EIgLoMVUd9oTU")
electronic <- get_playlist_audio_features("", "37i9dQZF1DX0wMD4IoQ5aJ")
studying <- get_playlist_audio_features("", "7ex5hFjCiAdaXy6FFoe2Vi")

study <-
  indie <-
  bind_rows(
    pop %>% mutate(playlist = "Pop study") %>% slice_head(n = 50),
    classic %>% mutate(playlist = "Classical study") %>% slice_head(n = 50),
    electronic %>% mutate(playlist = "Electronic study") %>% slice_head(n = 50),
    studying %>% mutate(playlist = "My study playlist") %>% slice_head(n = 50)
  ) 

indie_features <-
  study %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r echo=FALSE}
indie_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = indie_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
indie_cv <- indie_features %>% vfold_cv(5)
```

```{r echo=FALSE}
forest_model <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("ranger", importance = "impurity")
synth_forest <- 
  workflow() %>% 
  add_recipe(indie_recipe) %>% 
  add_model(forest_model) %>% 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r echo=FALSE}
p1 <- workflow() %>% 
  add_recipe(indie_recipe) %>% 
  add_model(forest_model) %>% 
  fit(indie_features) %>% 
  pluck("fit", "fit", "fit") %>%
  ranger::importance() %>% 
  enframe() %>% 
  mutate(name = fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) + 
  geom_col(fill = "#7395AE") + 
  coord_flip() +
  labs(x = NULL, y = "") + 
  theme_study()
p2 <- synth_forest %>% get_conf_mat() %>% 
  autoplot(type = "heatmap") + 
  aes(title = "Random Forest Truthtable") +
  theme_study() + 
  scale_fill_gradient(low = "#2E9CCA", high = "#25274D")
subplot(ggplotly(p1, legend = FALSE), ggplotly(p2), nrows = 1, margin = 0.06, titleX = TRUE, titleY = TRUE) %>% 
  layout(title = '', showlegend = FALSE) %>%
  layout(annotations = 
           list(list(x = 0.15 , y = 1.05, text = "Most Important Features",
                     showarrow = F, xref='paper', yref='paper', size = 22/.pt),
                list(x = 0.85 , y = 1.05, text = "Random Forest Truth Table", 
                     showarrow = F, xref='paper', yref='paper', size = 14/.pt))
  )
```


***

**Random Forest clustering**

After all previous analysis I finally compare my own study playlist with the playlists from spotify.
For this classifier I used a Random Forest classifier that distinguishes the four different playlists.
Validation is done using K-fold validation, using 5 partitions.

The classifier is plotted at the right, and the most important features are plotted at the right. 
For this classifier I used a n of 50, my own playlist and the classical study playlist both consist of exactly 55 tracks so this way each playlist consists of the same amount of tracks.

**Results**

With the results the maximum result is 50, this should be kept in mind while interpreting the plots. 
The classifier is (almost) perfect for the three spotify playlist. This is not a surprise, all three playlists consist of one theme. From my own study playlist 27 songs are classified correctly. This is a lot higher than a random guess (12.5) and actually higher than I expected. Most “mistakes” are identified as classical music tracks which is not weird. The tracks have clear features that makes them classical and different from the rest in the eyes of Spotify (high instrumentallness, low danceability etc.). And I also think most numbers are classical in my playlist compared to the other themes. 


### Conclusion {data-commentary-width=400}

**Goal**

My goal in this project was to compare my own study playlist with three study playlists from spotify. I knew that my own playlists don’t has a clear theme but more or less consists of classical study tracks, pop study tracks and techno study tracks. So that is exactly what I compared my study playlist against. There is no good or bad study music, everyone is different and has their own unique music taste. 

**Conclusion**

Most analysis gave me insight about music and tracks itself but did not compare the playlist as a whole. The useful analysis for this was the track features and the Random Forest classification. The plots from the track features showed what I expected; the tracks in Spotify's playlist are clearly clustered and the tracks in my own playlist are not.

I’m surprised that most of the songs in my playlist are classified correctly because that hints at the fact that there is some uniqueness in my tracks while I thought that almost all would also fit in other themes. I’m actually really keen to know what this is, my hypothesis is that this “feature” is that most songs have actually a bit of all three worlds. While there are some songs that are 100% techno or classical, most songs have elements from the three different themes. 

**Reflection and further research**

I’m actually really happy with my results; differences and similarities are both clearly visible. 
Beside the goal of comparing playlists I have also done a lot of other different analysis, most were more track focussed and while these did not give me insight in the playlist they gave me insights in the track. For me it was really new to look at music at this level, my musical background is very little so I learned a lot. I can look at (study) music with a new perspective which I’m really happy about. I loved the course and would recommend it to everyone. 
***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/7ex5hFjCiAdaXy6FFoe2Vi?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
