---
title: "Musicology_Portfolio"
author: "Ragnar van Dort"
date: "2/22/2022"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: flatly
    
---
```{r setup}
  library(tidyverse)
  library(plotly)
  library(spotifyr)
  library(compmus)
```

### The best music for studying

The main goal of this corpus is to compare playlists, to be specific, compare study playlists. A lot of people study, and a lot of people study while listening to music. All people are different and we also see this in their music taste; it's often said that classical music is great for studying and this is also backed by different studies but there also a lot of other kinds of music that students listen to: LoFi, techno etc.

Over the years I created a study playlist myself, it consist of roughly 3 genres I think: some songs with text but they are not to energetic and are a bit "slow", some techno music and some classical music. I highly doubt it's te the best playlist for studying, but over the years I know that I studied with these songs on. Maybe some songs are not ideal itself to listen to during studying but they help me get in a study mode. Maybe good to know due to it's variety its more a playlist that I use as a storage space for study music than that I actually play the playlist while studying. 


In this corpus I will compare my study playlist with some popular spotify study playlists, I'm curious how different my playlist is compared to a "normal" study playlist. 

I will work with four different playlists all created by spotify: a pop study playlist, a classical music study playlist, a techno study playlist and a LoFi study playlist.

I will compare my own study playlist (stu(dying)) with some popular spotify playlists. For the best comparison I decided to take three spotify playlists with the genres that are also mainly aanwezig in my own playlist:

- Pop Study (sudy music with text): https://open.spotify.com/playlist/37i9dQZF1DWSoyxGghlqv5 
- Classical Studying music: https://open.spotify.com/playlist/37i9dQZF1EIgLoMVUd9oTU 
- Electronic Focus: https://open.spotify.com/playlist/37i9dQZF1DX0wMD4IoQ5aJ 
- My own study music playlist: https://open.spotify.com/playlist/7ex5hFjCiAdaXy6FFoe2Vi

My hypothesis for this research is that there will be a lot of similarities, probably a low bpm and not too energetic for example. But I hope to find some interesting differences that I can then investigate further and possibly explain.

There are a few interesting and atypecal tracks in the playlist. There are of course some tracks that feature in my own playlist and also in spotify's playlist (Una mattine, Comptine d'un autre été: L'après-midi) altough they are a different version. Maybe some atypical music are the movie soundtracks that are in my playlist like: Time-Hans zimmer and the Godfather finale- Nino Rotta.

### Track level features

#### __The Tempo and Instrumentalness in study music__

```{r tempo and intrumentalness}
pop <- get_playlist_audio_features("", "37i9dQZF1DWSoyxGghlqv5")
classic <- get_playlist_audio_features("", "37i9dQZF1EIgLoMVUd9oTU")
electronic <- get_playlist_audio_features("", "37i9dQZF1DX0wMD4IoQ5aJ")
studying <- get_playlist_audio_features("", "7ex5hFjCiAdaXy6FFoe2Vi")

study <-
  pop %>%
  mutate(country = "Pop study") %>%
  bind_rows(classic %>% mutate(country = "Classical study")) %>%
  bind_rows(electronic %>% mutate(country = "Electronic study")) %>%
  bind_rows(studying %>% mutate(country = "Stu(dying)"))%>%
  mutate(
    country = fct_relevel(country, "Pop study, Classical study, Electronical study", "Stu(dying)")
  )

february_dip <-
  study %>%
  ggplot(                          # Set up the plot.
    aes(
      x = tempo,
      y = instrumentalness,
      size = track.popularity,
      colour = energy,
      label = track.name           # Labels will be interactively visible.
    )
  ) +
  geom_point() +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~country) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis. Outliers are not 
    limits = c(0, 200),
    breaks = c(0, 100, 200),        # Use grid-lines for quadrants only.
    minor_breaks = c(25, 50, 75, 125, 150, 175)            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
    guide = "none"
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
    guide = "none"                 # Remove the legend for size.
  ) +
  theme_light() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Tempo",
    y = "Instrumentalness"
  )

ggplotly(february_dip)
```
*** 

There is no consensus in the literature about what music is best for studying but every answer has one thing in common; the main goal is that it should help you to focus. Generally speaking music with a relative low bpm (<120 bpm) is best suited for this. In study music (new) texts also seem to be distractive, so a high instrumentalness would be a good sign. 

I plotted the instrumentalness on the y-axis, the tempo on the x-axis, the color is the energy of the song and the size is its popularity. 

The plots are kind of what I expected. The Spotify music playlists are pretty clustered while my own study playlist is not so much. This is not really hard to explain. The Spotify playlists consist of one genre while my playlist is a mix of genres. 

#### __The valence in the different playlistst__

```{r some plots}
library(dplyr)
library(spotifyr)
library(plotly)
library(ggplot2)

pop <- get_playlist_audio_features("", "37i9dQZF1DWSoyxGghlqv5")
classic <- get_playlist_audio_features("", "37i9dQZF1EIgLoMVUd9oTU")
electronic <- get_playlist_audio_features("", "37i9dQZF1DX0wMD4IoQ5aJ")
studying <- get_playlist_audio_features("", "7ex5hFjCiAdaXy6FFoe2Vi")

study <-
  pop %>%
  mutate(country = "Pop study") %>%
  bind_rows(classic %>% mutate(country = "Classical study")) %>%
  bind_rows(electronic %>% mutate(country = "Electronic study")) %>%
  bind_rows(studying %>% mutate(country = "Stu(dying)"))%>%
  mutate(
    country = fct_relevel(country, "Pop study, Classical study, Electronical study", "Stu(dying)")
  )
green <- "#1ed760"
yellow <- "#e7e247"
pink <- "#ff6f59"
blue <- "#17bebb"


viz4 <- ggplot(study, aes(x=valence, fill=playlist_name,
                            text = paste(playlist_name)))+
  geom_density(alpha=0.7, color=NA)+
  scale_fill_manual(values=c(green, yellow, pink, blue))+
  labs(x="Valence", y="Density") +
  guides(fill=guide_legend(title="country"))+
  theme_minimal()+
  ggtitle("Distribution of Valence Data")

ggplotly(viz4, tooltip=c("text"))

```

### Comptine d'un autre été: L'après-midi[chroma features].

```{r tallis}
lavinia <-
  get_tidy_audio_analysis("06MWWxWRuBtYxcJP71tN0q") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
tierssen <-
  get_tidy_audio_analysis("14rZjW3RioG7WesZhYESso") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
maria_dist <-
  compmus_long_distance(
    lavinia %>% mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
    tierssen %>% mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
    feature = pitches,
    method = "cosine"
  )
```

```{r tallis-plot}
maria <-
  maria_dist %>%
  mutate(
    lavinia = xstart + xduration / 2,
    tierssen = ystart + yduration / 2
  ) %>%
  ggplot(
    aes(
      x = lavinia,
      y = tierssen,
      fill = d
    )
  ) +
  geom_tile(aes(width = xduration, height = yduration)) +
  coord_fixed() +
  scale_x_continuous(
    breaks = c(0, 11, 29, 45, 65, 82, 100, 119, 126),
    labels =
      c(
        "Melody left hand",
        "melody 1 ",
        "Melody 2",
        "Melody",
        "Melody 1, octave higher",
        "Melody 2, octave higher",
        "Melody 3, octave higher",
        "fades out",
        ""
      ),
  ) +
  scale_y_continuous(
    breaks = c(0, 11, 31, 49, 69, 90, 110, 131, 140),
    labels =
      c(
        "Melody left hand",
        "melody 1 ",
        "Melody 2",
        "Melody",
        "Melody 1, octave higher",
        "Melody 2, octave higher",
        "Melody 3, octave higher",
        "fades out",
        ""
      ),
  ) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x = "Lavinia meijers", y = "Yann Tierssen")
maria
```

***

This is a visualization of two performances of the piano piece " Comptine d'un autre été - l'après midi" a performance of Lavina meijers, and the original performance by Yann Tierssen. I chose this music piece because the version of Lavina Meijers was in my playlist, and the version by Yann Tierssen was in the Classical study music playlist. 

It's the soundtrack of the French film Amelie and became popular ever since. It's a piano piece and notice that the left melody of the left hand part never changes (except from intensity). The right hand part has three distinctive melodies in a row that have different characteristics: The first one is catchy and has some mild dissonance, the second has fewer notes and leaves more space, and the third is a flurry of repeated triplets. Halfway through the song, the right hand jumps up an octave and plays the exact same thing as it did in the first half.

The version of Yann Tierssen is 16 seconds longer than the version of lavina meijers (2:22, 2:06). I have to look at his again but I can't distinguish this immediately. I think this is not that that the version of Lavina is a bit faster but its more that it starts earlier and has last pause between the different pieces of the music/less fade. This explains the repetition that we can clearly see in the plot. 

### The Godfather finale [chroma and timbre features]

```{r godfather}
bzt <-
  get_tidy_audio_analysis("78Q2Em989i9lagCBf8XGyX") %>% 
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "mean"
      )
  )
bind_rows(
  bzt %>% 
    compmus_self_similarity(pitches, "cosine") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  bzt %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***

The two self-similarity matrices at the right, each summarized at the bar level but with axes in seconds, illustrate pitch- and timbre-based self-similarity within Nini Rotta’s  famous The Godfather Finale. The chroma-based matrix shows us that the piece exists of two parts, divided around the 110 sec. But furthermore the chroma similarities seems to be all over the place, although you can distinguish the violin part (110-160 sec). This part is also very visible in the Timbre-based matrix.

### Chordogram and Keygram Time Hans Zimmer.

#### __Chordogram__

```{r chordogram}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("6ZFbXIJkuI1dVNWvzJzown") %>%
  compmus_align(beats, segments) %>%
  select(beats) %>%
  unnest(beats) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan" # summary, norm
      )
  )
twenty_five %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "manhattan",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

# over het algemeen, hoe meer geel hoe duidelijker een akkoord wordt herkend, blauw is slecht herkennen
# 
```
#### __Keygram__
```{r}

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
twenty_five <-
  get_tidy_audio_analysis("6ZFbXIJkuI1dVNWvzJzown") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan" # summary, norm
      )
  )
twenty_five %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "manhattan",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

# over het algemeen, hoe meer geel hoe duidelijker een akkoord wordt herkend, blauw is slecht herkennen
# 

```
*** 

Chromagram of Hans Zimmer - time, yellow -> good key recognition, blue not so good recognition. We can see a clearly see a sideways z pattern in the chordogram. This is easy to explain, there is a repetition in the track that keeps coming back. In the keygram the key estimates are not correct, most of the song is misunderstood as G major, while it should be E minor. 

More detailed analyze will folow

### Novelty and Temp Runaway Kanye West [temporal features]

#### __Tempogram Runaway__

```{r}
get_tidy_audio_analysis('3DK6m7It6Pw857FcQftMds') %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_classic()

pata_pata <-
  get_tidy_audio_analysis("3DK6m7It6Pw857FcQftMds") %>%
  select(segments) %>%
  unnest(segments)

```
***

The Fourier-based tempogram at the left is an attempt to use Spotify's API to analyse the tempo of Kanye West "Runaway. Overall, Spotify estimates that the main tempo of this track is pretty low, around 87 BPM, but looking in more detail at the tempogram, we see vaguely the so-called *tempo octaves* +- 130 BPM. We also see that the tempo in the first 40 seconds is just below 100 BPM this is the "piano part" where just one tone is played repeatedly. In some freer-form sections of the piece, tempo estimation is almost impossible, this is clearly visible in the outro (the last 3 minutes of the song). Here Kanye ends the song with a three-minute outro of autotuned and overdriven wordless singing accompanied by string sections harmonizing with his vocals, his voice is Auto-Tuned and distorted beyond perceptibility. This is often interpreted as it is done on purpose and that is serves as a metaphor for how Kanye feels himself; unheared and the majority of people can't understand him.

#### __Novelty functions Runaway__

```{r}
pata_pata %>%
  mutate(loudness_max_time = start + loudness_max_time) %>%
  arrange(loudness_max_time) %>%
  mutate(delta_loudness = loudness_max - lag(loudness_max)) %>%
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")


pata_pata %>%
  arrange(start) %>%
  mutate(timbre = map2(timbre, lag(timbre), `-`)) %>%
  slice(-1) %>%
  compmus_gather_timbre() %>%
  group_by(start, duration) %>% 
  summarise(novelty = sum(log1p(pmax(value, 0)))) %>% 
  ggplot(aes(x = start + duration / 2, y = novelty)) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```

***

Three different novelty functions; Energy, Chroma, Timbre. Energy is the clearest as expected, chroma all over the place, Timbre is better than expected. More detailed analyze will follow. 
